//
//  EthicalFilter.swift
//  QuestMe
//
//  ğŸ“‚ æ ¼ç´å ´æ‰€:
//      QuestMe/Core/EthicalFilter.swift
//
//  ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„:
//      èª¹è¬—ä¸­å‚·/å˜²ç¬‘/ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–æ¤œçŸ¥ã¨å…±æ„Ÿãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨ã€‚
//      - ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã€‚
//      - å¿œç­”ã®å®‰å…¨åŒ–ï¼ˆãƒ–ãƒ­ãƒƒã‚¯/æ³¨æ„/è¨€ã„æ›ãˆï¼‰ã€‚
//
//  ğŸ”— ä¾å­˜:
//      - Models.swift
//      - Foundation
//      - SwiftUI
//
//  ğŸ‘¤ è£½ä½œè€…: æ´¥æ‘ æ·³ä¸€
//  ğŸ“… åˆ¶ä½œæ—¥: 2025å¹´10æœˆ12æ—¥
//

import Foundation
import Combine
import SwiftUI

struct EthicalScore {
    let aggression: Int
    let mockery: Int
    let sensitivity: Int
    var total: Int { aggression + mockery + sensitivity }
}

final class EthicalFilter: ObservableObject {
    private let aggressionWords = ["ãƒã‚«","æ­»ã­","æ®ºã™","æ„šã‹","ã‚¯ã‚º","ã‚ã»","é–“æŠœã‘","dis","ä¾®è¾±"]
    private let mockeryWords = ["ç¬‘","è‰","å˜²ç¬‘","æ™’ã™","ç…½ã‚‹","ã„ã˜ã‚‹"]
    private let sensitiveWords = ["äº‹æ•…","ç½å®³","ç—…æ°—","è‡ªæ®º","é¬±","æš´åŠ›","å·®åˆ¥","è™å¾…","ã„ã˜ã‚","èª¹è¬—ä¸­å‚·"]

    private let replaceMap: [String:String] = [
        "æ­»ã­":"ï¼Šï¼Š","æ®ºã™":"ï¼Šï¼Š","ãƒã‚«":"ï¼Šï¼Š","ã‚¯ã‚º":"ï¼Šï¼Š","å˜²ç¬‘":"ï¼Šï¼Š","å·®åˆ¥":"ï¼Šï¼Š"
    ]

    private let blockThreshold = 3
    private let cautionThreshold = 2

    private let empathyTemplates: [ConversationSubject:[String]] = [
        .anxiety: [
            "ä¸å®‰ã«æ„Ÿã˜ã‚‹ã®ã¯è‡ªç„¶ãªã“ã¨ã§ã™ã€‚",
            "ã¾ãšçŠ¶æ³ã‚’æ•´ç†ã—ã¦ã€ä¸€æ­©ãšã¤é€²ã‚ã¾ã—ã‚‡ã†ã€‚"
        ],
        .life: [
            "æ—¥ã€…ã®ç”Ÿæ´»ã¯ã€ã¨ãã«è² è·ãŒé‡ãªã‚Šã¾ã™ã­ã€‚",
            "å°ã•ãªå‰é€²ã§ã‚‚å¤§åˆ‡ã«ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚"
        ],
        .health: [
            "ä½“èª¿ã‚’ç¬¬ä¸€ã«ã€ç„¡ç†ã®ãªã„ãƒšãƒ¼ã‚¹ã‚’å¤§åˆ‡ã«ã€‚",
            "å¿…è¦ãªã‚‰ã€å°‚é–€å®¶ã®åŠ©è¨€ã‚‚æ¤œè¨ã—ã¾ã—ã‚‡ã†ã€‚"
        ],
        .work: [
            "ä»•äº‹ã®è² è·ã¯ã€æ™‚æœŸã«ã‚ˆã£ã¦å¤§ããå¤‰ã‚ã‚Šã¾ã™ã­ã€‚",
            "å„ªå…ˆé †ä½ã‚’ã¤ã‘ã¦ã€ã§ãã‚‹ç¯„å›²ã‹ã‚‰æ•´ãˆã¾ã—ã‚‡ã†ã€‚"
        ],
        .politics: [
            "è¤‡é›‘ãªå•é¡Œã»ã©ã€è¤‡æ•°ã®è¦–ç‚¹ãŒå½¹ç«‹ã¡ã¾ã™ã€‚",
            "äº‹å®Ÿã¨è§£é‡ˆã‚’åˆ†ã‘ã¦è€ƒãˆã‚‹ã®ãŒæœ‰åŠ¹ã§ã™ã€‚"
        ],
        .entertainment: [
            "è©±é¡Œã‚’å…±æœ‰ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã€‚",
            "æƒ…å ±æºã®ç¢ºèªã‚‚å¿˜ã‚Œãšã«é€²ã‚ã¾ã™ã­ã€‚"
        ]
    ]

    func sanitizeUserInput(_ text: String) -> String {
        var out = text
        for (k,v) in replaceMap { out = out.replacingOccurrences(of: k, with: v) }
        return out
    }

    func score(_ text: String) -> EthicalScore {
        let l = text.lowercased()
        let aggr = aggressionWords.reduce(0) { $0 + (l.contains($1.lowercased()) ? 1 : 0) }
        let mock = mockeryWords.reduce(0) { $0 + (l.contains($1.lowercased()) ? 1 : 0) }
        let sens = sensitiveWords.reduce(0) { $0 + (l.contains($1.lowercased()) ? 1 : 0) }
        return EthicalScore(aggression: aggr, mockery: mock, sensitivity: sens)
    }

    func safeReply(from text: String) -> String {
        var reply = text
        for (k,v) in replaceMap { reply = reply.replacingOccurrences(of: k, with: v) }
        let l = reply.lowercased()
        if mockeryWords.contains(where: { l.contains($0.lowercased()) }) {
            reply = reply.replacingOccurrences(of: "ç¬‘", with: "")
            reply = reply.replacingOccurrences(of: "è‰", with: "")
            reply = "æ•¬æ„ã‚’ã‚‚ã£ã¦æƒ…å ±ã‚’å…±æœ‰ã—ã¾ã™ã€‚" + reply
        }
        return reply
    }

    func shouldBlock(_ score: EthicalScore) -> Bool {
        score.total >= blockThreshold || score.aggression > 0
    }

    func shouldCaution(_ score: EthicalScore) -> Bool {
        score.total >= cautionThreshold || score.sensitivity > 0
    }

    func empatheticPrefix(for topic: ConversationSubject) -> String {
        if let set = empathyTemplates[topic], let pick = set.randomElement() { return pick }
        return "è©±é¡Œã‚’å…±æœ‰ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã€‚"
    }
}
