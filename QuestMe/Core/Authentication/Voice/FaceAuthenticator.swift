//
//  FaceAuthenticator.swift
//  QuestMe
//
//  ğŸ“‚ æ ¼ç´å ´æ‰€:
//      QuestMe/Core/Companion/FaceAuthenticator.swift
//
//  ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„:
//      é¡”ç”»åƒã‹ã‚‰128æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã€CompanionProfile.faceprintData ã«ä¿å­˜ãƒ»ç…§åˆã™ã‚‹ã€‚
//      - CoreMLãƒ¢ãƒ‡ãƒ«ï¼ˆFaceRecognitionModel.mlpackageï¼‰ã‚’ä½¿ç”¨ã€‚
//      - CompanionSetupView ã§ç™»éŒ²ã€èªè¨¼æ™‚ã«ç…§åˆã€‚
//      - é¡ä¼¼åº¦è¨ˆç®—ã«ã‚ˆã‚Šæœ¬äººç¢ºèªã‚’è¡Œã†ã€‚
//
//  ğŸ”— ä¾å­˜:
//      - CompanionProfile.swiftï¼ˆfaceprintData: Data?ï¼‰
//      - CoreMLï¼ˆFaceRecognitionModelï¼‰
//      - Visionï¼ˆCVPixelBufferå‡¦ç†ï¼‰
//
//  ğŸ‘¤ è£½ä½œè€…: æ´¥æ‘ æ·³ä¸€
//  ğŸ“… åˆ¶ä½œæ—¥: 2025å¹´10æœˆ11æ—¥
//

import UIKit
import CoreML
import Vision

final class FaceAuthenticator {
    
    private let model: FaceRecognitionModel
    
    init?() {
        guard let model = try? FaceRecognitionModel(configuration: MLModelConfiguration()) else {
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return nil
        }
        self.model = model
    }
    
    /// é¡”ç”»åƒï¼ˆCVPixelBufferï¼‰ã‹ã‚‰128æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º
    func extractFaceEmbedding(from pixelBuffer: CVPixelBuffer) -> [Float]? {
        guard let resizedBuffer = resizeAndNormalize(pixelBuffer: pixelBuffer, targetSize: CGSize(width: 160, height: 160)) else {
            print("âŒ å…¥åŠ›ç”»åƒã®å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return nil
        }
        
        guard let output = try? model.prediction(x_1: resizedBuffer) else {
            print("âŒ æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return nil
        }
        
        let embedding = output.var_2167
        return (0..<embedding.count).map { Float(truncating: embedding[$0]) }
    }
    
    /// CompanionProfile ã«é¡”ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜ï¼ˆDataå½¢å¼ï¼‰
    func registerFace(from pixelBuffer: CVPixelBuffer, to profile: inout CompanionProfile) {
        guard let embedding = extractFaceEmbedding(from: pixelBuffer) else {
            print("âŒ é¡”ãƒ™ã‚¯ãƒˆãƒ«ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        }
        profile.faceprintData = Data(buffer: UnsafeBufferPointer(start: embedding, count: embedding.count))
        print("âœ… é¡”ãƒ™ã‚¯ãƒˆãƒ«ã‚’ CompanionProfile ã«ä¿å­˜ã—ã¾ã—ãŸ")
    }
    
    /// CompanionProfile ã®é¡”ãƒ™ã‚¯ãƒˆãƒ«ã¨ç…§åˆï¼ˆCosineé¡ä¼¼åº¦ï¼‰
    func authenticateFace(from pixelBuffer: CVPixelBuffer, with profile: CompanionProfile, threshold: Float = 0.5) -> Bool {
        guard let inputEmbedding = extractFaceEmbedding(from: pixelBuffer),
              let storedData = profile.faceprintData else {
            print("âŒ é¡”ãƒ™ã‚¯ãƒˆãƒ«ã®ç…§åˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return false
        }
        
        let storedEmbedding = storedData.withUnsafeBytes {
            Array(UnsafeBufferPointer<Float>(start: $0.baseAddress!.assumingMemoryBound(to: Float.self),
                                             count: storedData.count / MemoryLayout<Float>.size))
        }
        
        let similarity = cosineSimilarity(inputEmbedding, storedEmbedding)
        print("ğŸ” é¡ä¼¼åº¦: \(similarity)")
        return similarity >= threshold
    }
    
    /// Cosineé¡ä¼¼åº¦ã‚’è¨ˆç®—
    private func cosineSimilarity(_ a: [Float], _ b: [Float]) -> Float {
        let dot = zip(a, b).map(*).reduce(0, +)
        let normA = sqrt(a.map { $0 * $0 }.reduce(0, +))
        let normB = sqrt(b.map { $0 * $0 }.reduce(0, +))
        return dot / (normA * normB)
    }
    
    /// CoreMLãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã¦ç”»åƒã‚’160x160ã«ãƒªã‚µã‚¤ã‚ºã—ã€RGBæ­£è¦åŒ–
    private func resizeAndNormalize(pixelBuffer: CVPixelBuffer, targetSize: CGSize) -> CVPixelBuffer? {
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let resizedImage = ciImage.transformed(by: CGAffineTransform(scaleX: targetSize.width / CGFloat(CVPixelBufferGetWidth(pixelBuffer)),
                                                                     y: targetSize.height / CGFloat(CVPixelBufferGetHeight(pixelBuffer))))
        
        let context = CIContext()
        var outputBuffer: CVPixelBuffer?
        let attrs = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ] as CFDictionary
        
        let status = CVPixelBufferCreate(kCFAllocatorDefault,
                                         Int(targetSize.width),
                                         Int(targetSize.height),
                                         kCVPixelFormatType_32BGRA,
                                         attrs,
                                         &outputBuffer)
        
        guard status == kCVReturnSuccess, let buffer = outputBuffer else {
            return nil
        }
        
        context.render(resizedImage, to: buffer)
        return buffer
    }
}
