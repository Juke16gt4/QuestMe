//
//  SpeechRecognizer.swift
//  QuestMe
//
//  üìÇ Ê†ºÁ¥çÂ†¥ÊâÄ:
//      QuestMe/Services/Speech/SpeechRecognizer.swift
//
//  üéØ „Éï„Ç°„Ç§„É´„ÅÆÁõÆÁöÑ:
//      Èü≥Â£∞ÂÖ•Âäõ„Çí„ÉÜ„Ç≠„Çπ„Éà„Å´Â§âÊèõ„Åô„Çã„Çµ„Éº„Éì„Çπ„ÄÇ
//      - start(handler:) „ÅßÈü≥Â£∞Ë™çË≠ò„ÇíÈñãÂßã„Åó„ÄÅÁµêÊûú„Çí„ÇØ„É≠„Éº„Ç∏„É£„ÅßËøî„Åô„ÄÇ
//      - stop() „ÅßÈü≥Â£∞Ë™çË≠ò„ÇíÂÅúÊ≠¢„ÄÇ
//      - iOS Speech Framework „ÇíÂà©Áî®„Åó„ÄÅÈÅ∏ÊäûË®ÄË™û„Å´Âøú„Åò„Å¶Ë™çË≠ò„ÄÇ
//      - CompanionOverlay „ÇÑ UserProfileView „Åã„ÇâÂà©Áî®„Åï„Çå„Çã„ÄÇ
//
//  üîó ‰æùÂ≠ò:
//      - AVFoundation
//      - Speech
//
//  üë§ Ë£Ω‰ΩúËÄÖ: Ê¥•Êùë Ê∑≥‰∏Ä
//  üìÖ ÊúÄÁµÇÊõ¥Êñ∞: 2025Âπ¥10Êúà17Êó•
//

import Foundation
import Speech
import AVFoundation

final class SpeechRecognizer: NSObject, ObservableObject {
    static let shared = SpeechRecognizer()
    private override init() {}

    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var speechRecognizer: SFSpeechRecognizer?

    @Published var transcript: String = ""

    /// Èü≥Â£∞Ë™çË≠ò„ÅÆÊ®©Èôê„Çí„É™„ÇØ„Ç®„Çπ„Éà
    func requestAuthorization(completion: @escaping (Bool) -> Void) {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                completion(status == .authorized)
            }
        }
    }

    /// Èü≥Â£∞Ë™çË≠ò„ÇíÈñãÂßã
    /// - Parameters:
    ///   - locale: Ë®ÄË™û„Ç≥„Éº„ÉâÔºà‰æã: "ja-JP", "en-US"Ôºâ
    ///   - handler: Ë™çË≠òÁµêÊûú„ÇíËøî„Åô„ÇØ„É≠„Éº„Ç∏„É£
    func start(locale: String = "ja-JP", handler: @escaping (String) -> Void) throws {
        stop() // ÂâçÂõû„ÅÆ„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíÂÅúÊ≠¢

        self.speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: locale))
        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            throw NSError(domain: "SpeechRecognizer", code: -1, userInfo: [NSLocalizedDescriptionKey: "Èü≥Â£∞Ë™çË≠ò„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì"])
        }

        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        recognitionRequest.shouldReportPartialResults = true

        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.removeTap(onBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }

        audioEngine.prepare()
        try audioEngine.start()

        recognitionTask = recognizer.recognitionTask(with: recognitionRequest) { result, error in
            if let result = result {
                let best = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.transcript = best
                    handler(best)
                }
            }
            if error != nil {
                self.stop()
            }
        }
    }

    /// Èü≥Â£∞Ë™çË≠ò„ÇíÂÅúÊ≠¢
    func stop() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
    }
}
