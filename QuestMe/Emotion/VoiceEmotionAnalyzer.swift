//
//  VoiceEmotionAnalyzer.swift
//  QuestMe
//
//  ðŸ“‚ æ ¼ç´å ´æ‰€:
//      QuestMe/Emotion/VoiceEmotionAnalyzer.swift
//
//  ðŸŽ¯ ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„:
//      ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°å…¥åŠ›ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è§£æžã—ã€æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ï¼ˆä¾‹ï¼šneutral, joyï¼‰ã‚’æŽ¨å®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã€‚
//      - AVAudioEngine ã‚’ç”¨ã„ã¦éŸ³å£°å–å¾—ã€‚
//      - Combine ã«æº–æ‹ ã—ã€SwiftUI ã‹ã‚‰ @Published detectedEmotion ã‚’ç›£è¦–å¯èƒ½ã€‚
//      - Companion ã®å¿œç­”ã‚„è¡¨æƒ…å¤‰åŒ–ã«é€£å‹•ã€‚
//      - ConsentManager ã«ã¦ emotionInference ã®åŒæ„ãŒã‚ã‚‹å ´åˆã®ã¿èµ·å‹•å¯èƒ½ã€‚
//
//  ðŸ”— ä¾å­˜:
//      - AVFoundationï¼ˆéŸ³å£°å–å¾—ï¼‰
//      - Combineï¼ˆçŠ¶æ…‹ç›£è¦–ï¼‰
//      - CompanionExpression.swiftï¼ˆè¡¨æƒ…é€£å‹•ï¼‰
//      - ConsentManager.swiftï¼ˆåŒæ„ç¢ºèªï¼‰
//
//  ðŸ‘¤ è£½ä½œè€…: æ´¥æ‘ æ·³ä¸€
//  ðŸ“… åˆ¶ä½œæ—¥: 2025å¹´9æœˆ30æ—¥

import Foundation
import AVFoundation
import Combine

public final class VoiceEmotionAnalyzer: ObservableObject {
    private let engine = AVAudioEngine()
    @Published public var detectedEmotion: String = "neutral"

    public init() {}

    public func startListening() {
        let inputNode = engine.inputNode
        let format = inputNode.outputFormat(forBus: 0)

        inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
            let emotion = self.analyze(buffer: buffer)
            DispatchQueue.main.async {
                self.detectedEmotion = emotion
            }
        }

        try? engine.start()
    }

    public func stopListening() {
        engine.stop()
        engine.inputNode.removeTap(onBus: 0)
    }

    private func analyze(buffer: AVAudioPCMBuffer) -> String {
        // ä»®ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼šéŸ³é‡ã§æ„Ÿæƒ…ã‚’åˆ¤å®š
        let level = buffer.frameLength
        return level > 500 ? "joy" : "neutral"
    }
}
